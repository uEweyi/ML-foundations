{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jonkrohn/ML-foundations/blob/master/notebooks/5-probability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTOLgsbN69-P"
   },
   "source": [
    "# Probability & Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yqUB9FTRAxd-"
   },
   "source": [
    "This class, *Probability & Information Theory*, introduces the mathematical fields that enable us to quantify uncertainty as well as to make predictions despite uncertainty. These fields are essential because machine learning algorithms are both trained by imperfect data and deployed into noisy, real-world scenarios they haven’t encountered before. \n",
    "\n",
    "Through the measured exposition of theory paired with interactive examples, you’ll develop a working understanding of variables, probability distributions, metrics for assessing distributions, and graphical models. You’ll also learn how to use information theory to measure how much meaningful signal there is within some given data. The content covered in this class is itself foundational for several other classes in the *Machine Learning Foundations* series, especially *Intro to Statistics* and *Optimization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4tBvI88BheF"
   },
   "source": [
    "Over the course of studying this topic, you'll: \n",
    "\n",
    "* Develop an understanding of what’s going on beneath the hood of predictive statistical models and machine learning algorithms, including those used for deep learning. \n",
    "* Understand the appropriate variable type and probability distribution for representing a given class of data, as well as the standard techniques for assessing the relationships between distributions.\n",
    "* Apply information theory to quantify the proportion of valuable signal that’s present amongst the noise of a given probability distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z68nQ0ekCYhF"
   },
   "source": [
    "**Note that this Jupyter notebook is not intended to stand alone. It is the companion code to a lecture or to videos from Jon Krohn's [Machine Learning Foundations](https://github.com/jonkrohn/ML-foundations) series, which offer detail on the following:**\n",
    "\n",
    "*Segment 1: Introduction to Probability*\n",
    "* A Brief History of Probability Theory: Frequentists vs Bayesians\n",
    "* Applications of Probability to Machine Learning\n",
    "* Random vs Fixed Variables\n",
    "* Discrete vs Continuous Variables\n",
    "* Probability Mass and Probability Density Functions\n",
    "* Marginal and Conditional Probabilities\n",
    "* Expected Value\n",
    "* Measures of Central Tendency: Mean, Median, and Mode\n",
    "* Quantiles: Quartiles, Deciles, and Percentiles\n",
    "* The Box-and-Whisker Plot\n",
    "* Variance and Standard Deviation\n",
    "* Covariance and Correlation\n",
    "* Directed and Undirected Graphical Models\n",
    "\n",
    "*Segment 2: Distributions in Machine Learning*\n",
    "* Uniform\n",
    "* Gaussian: Normal and Standard Normal\n",
    "* Log-Normal\n",
    "* Poisson\n",
    "* Exponential and Laplace\n",
    "* Bernoulli, Multinomial and Multinoulli\n",
    "* Mixtures of Distributions\n",
    "* The Central Limit Theorem\n",
    "\n",
    "*Segment 3: Information Theory*\n",
    "* What Information Theory Is\n",
    "* Nats vs Bits\n",
    "* Shannon vs Differential Entropy\n",
    "* Kullback-Liebler Divergence\n",
    "* Cross-Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code coming in June 2020... Watch this space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO4toL+odzCdics69uQ9+W4",
   "include_colab_link": true,
   "name": "5-probability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
