{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jonkrohn/ML-foundations/blob/master/notebooks/8-optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTOLgsbN69-P"
   },
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sqs7yWgf_NQA"
   },
   "source": [
    "This class, *Optimization*, is the eighth of eight classes in the *Machine Learning Foundations* series. It builds upon the material from each of the other classes in the series -- on linear algebra, calculus, probability, statistics, and algorithms -- in order to provide a detailed introduction to training machine learning models. \n",
    "\n",
    "Through the measured exposition of theory paired with interactive examples, you’ll develop a working understanding of all of the essential theory behind the ubiquitous gradient descent approach to optimization as well as how to apply it yourself -- both at a granular, matrix operations level and a quick, abstract level -- with TensorFlow and PyTorch. You’ll also learn about the latest optimizers, such as Adam and Nadam, that are widely-used for training deep neural networks. \n",
    "\n",
    "Over the course of studying this topic, you'll:\n",
    "\n",
    "* Discover how the statistical and machine learning approaches to optimization differ, and why you would select one or the other for a given problem you’re solving.\n",
    "* Find out how the extremely versatile (stochastic) gradient descent optimization algorithm works, including how to apply it -- from a low, in-depth level as well as from a high, abstracted level -- within the most popular deep learning libraries, Tensorflow and PyTorch\n",
    "* Get acquainted with the “fancy” optimizers that are available for advanced machine learning approaches (e.g., deep learning) and when you should consider using them.\n",
    "\n",
    "Note that this Jupyter notebook is not intended to stand alone. It is the companion code to a lecture or to videos from Jon Krohn's [Machine Learning Foundations](https://github.com/jonkrohn/ML-foundations) series, which offer detail on the following:\n",
    "\n",
    "*Segment 1: The Machine Learning Approach to Optimization*\n",
    "\n",
    "* The Statistical Approach to Regression: Ordinary Least Squares\n",
    "* When Statistical Approaches to Optimization Breakdown\n",
    "* The Machine Learning Solution \n",
    "\n",
    "*Segment 2: Gradient Descent*\n",
    "\n",
    "* Objective Functions\n",
    "* Cost / Loss / Error Functions\n",
    "* Minimizing Cost with Gradient Descent\n",
    "* The Global Minimum and Local Minima\n",
    "* Saddle Points\n",
    "* Maximizing Reward with Gradient Ascent \n",
    "* Learning Rate\n",
    "* Point-by-Point Regression in TensorFlow / PyTorch\n",
    "* Tensor-Based Regression in TensorFlow / PyTorch\n",
    "* Mini-Batches and Stochastic Gradient Descent (SGD)\n",
    "* Regression with SGD in TensorFlow / PyTorch\n",
    "* The Jacobian and the Hessian\n",
    "\n",
    "*Segment 3: Fancy Deep Learning Optimizers*\n",
    "\n",
    "* Momentum\n",
    "* Nesterov Momentum\n",
    "* AdaGrad\n",
    "* AdaDelta \n",
    "* RMSProp\n",
    "* Adam \n",
    "* Nadam\n",
    "* A Layer of Artificial Neurons in TensorFlow / PyTorch\n",
    "* Training a Deep Neural Net with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-EthP5O_NQB"
   },
   "source": [
    "## Segment 1: Optimization Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the *Ordinary Least Squares* section of the [*Intro to Stats* notebook](https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/6-statistics.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment 2: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, for some instance $i$, we'd like to quantify the difference between the \"correct\" target model output $y_i$ and the model's predicted output $\\hat{y}_i$. A first idea might be to take the simple difference: $$\\Delta y_i = \\hat{y}_i - y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational efficiency (we'll explore this later in the segment), in ML we seldom consider the cost associated with a single instance $i$. Instead, we typically consider several instances simultaneously, in which case calculating the simple difference is largely  ineffective because positive and negative $\\Delta y_i$ values cancel out. E.g., consider a situation where: \n",
    "\n",
    "* $\\Delta y_1 = \\hat{y}_1 - y_1$ = 7 - 2 = 5\n",
    "* $\\Delta y_2 = \\hat{y}_2 - y_2$ = 3 - 8 = -5\n",
    "\n",
    "On an individual-instance basis, there are differences between the predicted and target outputs, indicating the model could be improved. Despite this, the total cost ($\\Sigma \\Delta y = \\Delta y_1 + \\Delta y_2 = 5-5$) is zero and therefore the mean cost ($\\frac{\\Sigma{\\Delta y}}{n} = \\frac{0}{2}$) is also zero, erroneously implying a perfect model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the following notebooks:\n",
    "    \n",
    "* [*Gradient Descent from Scratch*](https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/gradient-descent-from-scratch.ipynb)\n",
    "* [*SGD from Scratch*](https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/SGD-from-scratch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "8-optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
