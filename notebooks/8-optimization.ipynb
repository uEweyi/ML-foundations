{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8-optimization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonkrohn/ML-foundations/blob/master/notebooks/8-optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aTOLgsbN69-P"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqs7yWgf_NQA",
        "colab_type": "text"
      },
      "source": [
        "This class, *Optimization*, is the eighth of eight classes in the *Machine Learning Foundations* series. It builds upon the material from each of the other classes in the series -- on linear algebra, calculus, probability, statistics, and algorithms -- in order to provide a detailed introduction to training machine learning models. \n",
        "\n",
        "Through the measured exposition of theory paired with interactive examples, you’ll develop a working understanding of all of the essential theory behind the ubiquitous gradient descent approach to optimization as well as how to apply it yourself -- both at a granular, matrix operations level and a quick, abstract level -- with TensorFlow and PyTorch. You’ll also learn about the latest optimizers, such as Adam and Nadam, that are widely-used for training deep neural networks. \n",
        "\n",
        "Over the course of studying this topic, you'll:\n",
        "\n",
        "* Discover how the statistical and machine learning approaches to optimization differ, and why you would select one or the other for a given problem you’re solving.\n",
        "* Find out how the extremely versatile (stochastic) gradient descent optimization algorithm works, including how to apply it -- from a low, in-depth level as well as from a high, abstracted level -- within the most popular deep learning libraries, Tensorflow and PyTorch\n",
        "* Get acquainted with the “fancy” optimizers that are available for advanced machine learning approaches (e.g., deep learning) and when you should consider using them.\n",
        "\n",
        "Note that this Jupyter notebook is not intended to stand alone. It is the companion code to a lecture or to videos from Jon Krohn's [Machine Learning Foundations](https://github.com/jonkrohn/ML-foundations) series, which offer detail on the following:\n",
        "\n",
        "*Segment 1: The Machine Learning Approach to Optimization*\n",
        "\n",
        "* The Statistical Approach to Regression: Ordinary Least Squares\n",
        "* When Statistical Approaches to Optimization Breakdown\n",
        "* The Machine Learning Solution \n",
        "\n",
        "*Segment 2: Gradient Descent*\n",
        "\n",
        "* Objective Functions\n",
        "* Cost / Loss / Error Functions\n",
        "* Minimizing Cost with Gradient Descent\n",
        "* The Global Minimum and Local Minima\n",
        "* Saddle Points\n",
        "* Maximizing Reward with Gradient Ascent \n",
        "* Learning Rate\n",
        "* Point-by-Point Regression in TensorFlow / PyTorch\n",
        "* Tensor-Based Regression in TensorFlow / PyTorch\n",
        "* Mini-Batches and Stochastic Gradient Descent (SGD)\n",
        "* Regression with SGD in TensorFlow / PyTorch\n",
        "* The Jacobian and the Hessian\n",
        "\n",
        "*Segment 3: Fancy Deep Learning Optimizers*\n",
        "\n",
        "* Momentum\n",
        "* Nesterov Momentum\n",
        "* AdaGrad\n",
        "* AdaDelta \n",
        "* RMSProp\n",
        "* Adam \n",
        "* Nadam\n",
        "* A Layer of Artificial Neurons in TensorFlow / PyTorch\n",
        "* Training a Deep Neural Net with SGD\n",
        "\n",
        "**Code coming in August 2020... Watch this space.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-EthP5O_NQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
